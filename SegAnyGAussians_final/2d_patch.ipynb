{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982d02c1-3b7e-4c62-aadc-c003d78f905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import torch\n",
    "import pytorch3d.ops\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import cv2\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "\n",
    "#import utils.contrastive_decoder_utils\n",
    "from utils.sh_utils import SH2RGB\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bba4676-f83b-4568-9795-d3a0bc51b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32 # fixed\n",
    "\n",
    "# MODEL_PATH = './output/lerf-fruit_aisle/'\n",
    "MODEL_PATH = './output/lund_1024' # 30000\n",
    "\n",
    "FEATURE_GAUSSIAN_ITERATION = 10000\n",
    "\n",
    "SCALE_GATE_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scale_gate.pt')\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7827c4e-1620-44ce-b78e-77ee379296bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for config file in ./output/lund_1024/cfg_args\n",
      "Config file found: ./output/lund_1024/cfg_args\n",
      "Loading trained model at iteration 30000, 10000\n",
      "Allow Camera Principle Point Shift: False\n",
      "Reading camera 1196/1196\n",
      "âœ… Loaded 1196 cameras for this GPU (start_idx=0, end_idx=None)\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n"
     ]
    }
   ],
   "source": [
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate = scale_gate.cuda()\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument('--target', default='scene', type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "\n",
    "# If use language-driven segmentation, load clip feature and original masks\n",
    "dataset.need_features = True\n",
    "\n",
    "# To obtain mask scales\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efe150f-c62a-4649-8963-6ba5aad69131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension 512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import clip_utils\n",
    "importlib.reload(clip_utils)\n",
    "from clip_utils import get_scores_with_template\n",
    "from clip_utils.clip_utils import load_clip\n",
    "clip_model = load_clip()\n",
    "clip_model.eval()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7204b6-3e60-4128-891b-63893fda4acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.245272 ?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Borrowed from GARField, but modified\n",
    "def get_quantile_func(scales: torch.Tensor, distribution=\"normal\"):\n",
    "    \"\"\"\n",
    "    Use 3D scale statistics to normalize scales -- use quantile transformer.\n",
    "    \"\"\"\n",
    "    scales = scales.flatten()\n",
    "\n",
    "    scales = scales.detach().cpu().numpy()\n",
    "    print(scales.max(), '?')\n",
    "\n",
    "    # Calculate quantile transformer\n",
    "    quantile_transformer = QuantileTransformer(output_distribution=distribution)\n",
    "    quantile_transformer = quantile_transformer.fit(scales.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    def quantile_transformer_func(scales):\n",
    "        scales_shape = scales.shape\n",
    "\n",
    "        scales = scales.reshape(-1,1)\n",
    "        \n",
    "        return torch.Tensor(\n",
    "            quantile_transformer.transform(scales.detach().cpu().numpy())\n",
    "        ).to(scales.device).reshape(scales_shape)\n",
    "\n",
    "    return quantile_transformer_func, quantile_transformer\n",
    "    \n",
    "all_scales = []\n",
    "all_scales = []\n",
    "for cam in scene.getTrainCameras():\n",
    "    scale_path = os.path.join(dataset.source_path, 'mask_scales', cam.image_name + '.pt')\n",
    "    scales = torch.load(scale_path)\n",
    "    all_scales.append(scales)\n",
    "\n",
    "all_scales = torch.cat(all_scales)\n",
    "\n",
    "upper_bound_scale = all_scales.max().item()\n",
    "# upper_bound_scale = np.percentile(all_scales.detach().cpu().numpy(), 75)\n",
    "\n",
    "# all_scales = []\n",
    "# for cam in scene.getTrainCameras():\n",
    "#     cam.mask_scales = torch.clamp(cam.mask_scales, 0, upper_bound_scale).detach()\n",
    "#     all_scales.append(cam.mask_scales)\n",
    "# all_scales = torch.cat(all_scales)\n",
    "\n",
    "# quantile transformer\n",
    "q_trans, q_trans_ = get_quantile_func(all_scales, 'uniform')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe08d5d2-7911-417d-b706-6baa49780a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 31.73 GiB total capacity; 30.13 GiB already allocated; 294.94 MiB free; 31.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m scale_conditioned_feature \u001b[38;5;241m=\u001b[39m rendered_feature\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m scale_gates\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m scale_conditioned_feature \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(scale_conditioned_feature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m mask_features \u001b[38;5;241m=\u001b[39m (\u001b[43msam_masks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale_conditioned_feature\u001b[49m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\n\u001b[1;32m     50\u001b[0m     sam_masks\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-9\u001b[39m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m mask_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(mask_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     54\u001b[0m mask_ident \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnc,nc->n\u001b[39m\u001b[38;5;124m'\u001b[39m, mask_features, mask_features) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# dummy consistency\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 31.73 GiB total capacity; 30.13 GiB already allocated; 294.94 MiB free; 31.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "seg_features = []\n",
    "clip_features = []\n",
    "scales = []\n",
    "mask_identifiers = []\n",
    "camera_id_mask_id = []\n",
    "\n",
    "bg_color = [0 for _ in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "for i, view in enumerate(scene.getTrainCameras()):\n",
    "    torch.cuda.empty_cache()\n",
    "    clip_features.append(view.original_features)\n",
    "\n",
    "    tmp_view = deepcopy(view)\n",
    "    tmp_view.feature_height, tmp_view.feature_width = view.original_image.shape[-2:]\n",
    "\n",
    "    rendered_feature = render_contrastive_feature(\n",
    "        tmp_view,\n",
    "        feature_gaussians,\n",
    "        pipeline.extract(args),\n",
    "        background,\n",
    "        norm_point_features=True\n",
    "    )['render']\n",
    "\n",
    "    feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "    rendered_feature = torch.nn.functional.interpolate(\n",
    "        rendered_feature.unsqueeze(0), (feature_h // 4, feature_w // 4), mode='bilinear'\n",
    "    ).squeeze()\n",
    "\n",
    "    sam_masks = torch.load(view.original_masks).cuda().unsqueeze(1)\n",
    "    sam_masks = torch.nn.functional.interpolate(sam_masks.float(), (feature_h // 4, feature_w // 4), mode='bilinear')\n",
    "    sam_masks = torch.conv2d(\n",
    "        sam_masks.float().cpu(),\n",
    "        torch.full((3, 3), 1.0).view(1, 1, 3, 3).cpu(),\n",
    "        padding=1\n",
    "    )\n",
    "    sam_masks = sam_masks >= 2\n",
    "    sam_masks = sam_masks.cuda()\n",
    "\n",
    "    mask_scales = torch.load(view.mask_scales).cuda().unsqueeze(-1)\n",
    "    mask_scales = q_trans(mask_scales)\n",
    "    scale_gates = scale_gate(mask_scales)\n",
    "\n",
    "    scale_conditioned_feature = rendered_feature.unsqueeze(0) * scale_gates.unsqueeze(-1).unsqueeze(-1)\n",
    "    scale_conditioned_feature = torch.nn.functional.normalize(scale_conditioned_feature, dim=1, p=2)\n",
    "\n",
    "    mask_features = (sam_masks * scale_conditioned_feature).sum(dim=-1).sum(dim=-1) / (\n",
    "        sam_masks.sum(dim=-1).sum(dim=-1) + 1e-9\n",
    "    )\n",
    "    mask_features = torch.nn.functional.normalize(mask_features, dim=-1, p=2)\n",
    "\n",
    "    mask_ident = torch.einsum('nc,nc->n', mask_features, mask_features) > 0.5  # dummy consistency\n",
    "    mask_identifiers.append(mask_ident.cpu())\n",
    "    seg_features.append(mask_features)\n",
    "    scales.append(mask_scales)\n",
    "\n",
    "    for j in range(len(mask_features)):\n",
    "        camera_id_mask_id.append((i, j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b2c5b-2d21-4bc4-8fd3-278c9dccd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all features from all cameras\n",
    "flattened_mask_features = torch.cat(seg_features, dim=0)\n",
    "clip_features = [torch.load(path).cuda() for path in clip_features]\n",
    "flattened_clip_features = torch.cat(clip_features, dim=0)\n",
    "flattened_clip_features = torch.nn.functional.normalize(flattened_clip_features.float(), dim=-1, p=2)\n",
    "\n",
    "flattened_scales = torch.cat(scales, dim=0)\n",
    "flattened_mask_identifiers = torch.cat(mask_identifiers, dim=0).to(torch.float16).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554dbddc-db37-4392-afc2-aa1bf8064d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = load_clip()\n",
    "clip_model.eval()\n",
    "\n",
    "prompt = \"house\"\n",
    "scores = get_scores_with_template(clip_model, flattened_clip_features.cuda(), prompt).squeeze()\n",
    "\n",
    "best_idx = scores.argmax()\n",
    "query_feature = flattened_mask_features[best_idx]\n",
    "query_scale = flattened_scales[best_idx].item()\n",
    "\n",
    "similarities = get_similarity_map(\n",
    "    feature_gaussians.get_point_features,\n",
    "    query_scale,\n",
    "    scale_gate,\n",
    "    query_feature,\n",
    "    q_trans\n",
    ")\n",
    "\n",
    "scene_gaussians.roll_back()\n",
    "scene_gaussians.segment(similarities > 0.45)\n",
    "\n",
    "background = torch.tensor([1.0] * FEATURE_DIM, dtype=torch.float32, device='cuda')\n",
    "rendered = render(scene.getTrainCameras()[camera_index], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "\n",
    "plt.imshow(rendered.permute(1, 2, 0).detach().cpu())\n",
    "plt.axis('off')\n",
    "plt.title(f\"3D Segmentation via Prompt: '{prompt}'\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809e126-6f82-425e-9281-bd5595eafe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "camera_index = 17  # Camera to extract 2D patch from\n",
    "query_indices = [(100, 800)]  # Pixel positions (H, W)\n",
    "threshold = 0.75\n",
    "# --- 1. Extract 2D feature map from camera ---\n",
    "view = scene.getTrainCameras()[camera_index]\n",
    "tmp_view = deepcopy(view)\n",
    "tmp_view.feature_height, tmp_view.feature_width = view.original_image.shape[-2:]\n",
    "\n",
    "bg_color = [0 for _ in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "render_pkg = render_contrastive_feature(\n",
    "    tmp_view,\n",
    "    feature_gaussians,\n",
    "    pipeline.extract(args),\n",
    "    background,\n",
    "    norm_point_features=True\n",
    ")\n",
    "rendered_feature = render_pkg['render']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f0775-b86a-4a8e-afdd-ea1fb5387221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to match scale with SAM or visual inspection\n",
    "feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "rendered_feature = torch.nn.functional.interpolate(\n",
    "    rendered_feature.unsqueeze(0), (feature_h // 4, feature_w // 4), mode='bilinear'\n",
    ").squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915f73c-ff02-4485-b8be-1e56f74211a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Normalize and extract query features ---\n",
    "normed_features = torch.nn.functional.normalize(rendered_feature, dim=0, p=2).permute(1, 2, 0)  # [H, W, C]\n",
    "similarity_list = []\n",
    "\n",
    "for qi in query_indices:\n",
    "    y = int(qi[0] / view.image_height * normed_features.shape[0])\n",
    "    x = int(qi[1] / view.image_width * normed_features.shape[1])\n",
    "    qf = normed_features[y, x]  # [C]\n",
    "    sim = torch.einsum('C,HWC->HW', qf.cuda(), normed_features)\n",
    "    similarity_list.append(sim)\n",
    "\n",
    "final_similarity_2d = torch.stack(similarity_list, dim=0).max(dim=0)[0]  # [H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfcd5c-0e4b-4dbe-a4bd-ce7b1591e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Visualize 2D similarity and threshold mask ---\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(final_similarity_2d.detach().cpu(), cmap='hot')\n",
    "plt.title(\"2D Similarity Heatmap\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow((final_similarity_2d > threshold).detach().cpu())\n",
    "plt.title(\"Thresholded 2D Mask\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b4610-1e93-4463-b6fa-52fd8cc4e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_mask = render_pkg['visibility_filter']  # shape: [N_total], bool\n",
    "mask_flat = (final_similarity_2d > threshold).float().flatten()\n",
    "print(\"visible_mask shape:\", visible_mask.shape)\n",
    "print(\"Number of visible points:\", visible_mask.sum().item())\n",
    "print(\"final_similarity_2d shape:\", final_similarity_2d.shape)\n",
    "print(\"mask_flat shape:\", mask_flat.shape)\n",
    "\n",
    "# Safe fix: crop mask_flat to match visible count\n",
    "mask_flat = mask_flat[:visible_mask.sum()]\n",
    "\n",
    "full_mask = torch.zeros_like(visible_mask, dtype=torch.bool)\n",
    "full_mask[visible_mask] = mask_flat.bool()\n",
    "\n",
    "print(f\"âœ… Projected {full_mask.sum().item()} 2D-selected points into 3D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec369e-c979-4c0c-b0d5-c493b95dcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = scene.getTrainCameras()\n",
    "rendered = render(cameras[17], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered.permute(1, 2, 0).detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535e18f-d0e2-46ba-bd90-ae913db9f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Apply segmentation ---\n",
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass\n",
    "scene_gaussians.segment(full_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17ed05-3df6-4aa4-bcd0-a4dfeeffc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Render result ---\n",
    "background = torch.tensor([1.0]*FEATURE_DIM, dtype=torch.float32, device=\"cuda\")\n",
    "rendered = render(cameras[camera_index], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "\n",
    "plt.imshow(rendered.permute(1, 2, 0).detach().cpu())\n",
    "plt.axis('off')\n",
    "plt.title(\"3D Segmentation via 2D Patch Query\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5093746-dffc-4a1a-923d-9b7e152d0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Optional: Save for later rendering ---\n",
    "torch.save(full_mask.cpu(), './segmentation_res/precomputed_mask.pt')\n",
    "print(\"âœ… Saved precomputed_mask.pt for render.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4a73b-d830-48de-bb8e-d4ecf5d4f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def get_similarity_map(\n",
    "    point_features: torch.Tensor,\n",
    "    scale: float,\n",
    "    scale_gate: Callable,\n",
    "    clip_query_feature: torch.Tensor,\n",
    "    q_trans: Callable[[torch.Tensor], torch.Tensor]\n",
    "):\n",
    "    scale = torch.tensor([scale], device='cuda')\n",
    "    scale = q_trans(scale)\n",
    "    gates = scale_gate(scale).squeeze()\n",
    "    \n",
    "    scale_conditioned = point_features * gates.unsqueeze(0)\n",
    "    normed = torch.nn.functional.normalize(scale_conditioned, dim=-1, p=2)\n",
    "    similarities = torch.einsum('C,NC->N', clip_query_feature, normed)\n",
    "    \n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917a67a-2ba5-443a-868e-57b54d094e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract\n",
    "point_features = feature_gaussians.get_point_features\n",
    "point_features = torch.nn.functional.normalize(point_features, dim=-1, p=2)\n",
    "\n",
    "# Step 2: Pick a CLIP prompt\n",
    "from utils.clip_utils import get_scores_with_template\n",
    "prompt = \"house\"\n",
    "scores = get_scores_with_template(clip_model, point_features, prompt).squeeze()\n",
    "\n",
    "# Step 3: Use highest scoring point as query\n",
    "top_index = scores.argmax()\n",
    "clip_query_feature = point_features[top_index]\n",
    "scale = 0.5  # Or adapt from your clusters\n",
    "similarities = get_similarity_map(point_features, scale, scale_gate, clip_query_feature, q_trans)\n",
    "\n",
    "# Step 4: Threshold and segment\n",
    "threshold = 0.45\n",
    "scene_gaussians.roll_back()\n",
    "scene_gaussians.segment(similarities > threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8826b-c6ad-45db-85fa-30e942736e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = torch.tensor([1.0] * FEATURE_DIM, dtype=torch.float32, device='cuda')\n",
    "rendered = render(scene.getTrainCameras()[17], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered.permute(1, 2, 0).detach().cpu())\n",
    "plt.axis('off')\n",
    "plt.title(\"Language-Driven Segmentation\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
