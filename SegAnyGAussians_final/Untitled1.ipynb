{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2391f8-17f8-4cb9-920b-ba1e9de8d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import pytorch3d.ops\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import cv2\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "\n",
    "\n",
    "from utils.sh_utils import SH2RGB\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2221d7-c049-4929-82f1-6bb7e56fb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32 # fixed\n",
    "\n",
    "# MODEL_PATH = './output/lerf-fruit_aisle/'\n",
    "MODEL_PATH = './output/lund_1024' # 30000\n",
    "\n",
    "FEATURE_GAUSSIAN_ITERATION = 10000\n",
    "\n",
    "SCALE_GATE_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scale_gate.pt')\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d8c37-f589-41d1-834b-d11112950da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate = scale_gate.cuda()\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument('--target', default='scene', type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "\n",
    "# If use language-driven segmentation, load clip feature and original masks\n",
    "dataset.need_features = True\n",
    "\n",
    "# To obtain mask scales\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21022aa-1cfd-4813-a0c2-ba63eaec5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Borrowed from GARField, but modified\n",
    "def get_quantile_func(scales: torch.Tensor, distribution=\"normal\"):\n",
    "    \"\"\"\n",
    "    Use 3D scale statistics to normalize scales -- use quantile transformer.\n",
    "    \"\"\"\n",
    "    scales = scales.flatten()\n",
    "\n",
    "    scales = scales.detach().cpu().numpy()\n",
    "    print(scales.max(), '?')\n",
    "\n",
    "    # Calculate quantile transformer\n",
    "    quantile_transformer = QuantileTransformer(output_distribution=distribution)\n",
    "    quantile_transformer = quantile_transformer.fit(scales.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    def quantile_transformer_func(scales):\n",
    "        scales_shape = scales.shape\n",
    "\n",
    "        scales = scales.reshape(-1,1)\n",
    "        \n",
    "        return torch.Tensor(\n",
    "            quantile_transformer.transform(scales.detach().cpu().numpy())\n",
    "        ).to(scales.device).reshape(scales_shape)\n",
    "\n",
    "    return quantile_transformer_func, quantile_transformer\n",
    "    \n",
    "all_scales = []\n",
    "for cam in scene.getTrainCameras():\n",
    "    scale_path = os.path.join(dataset.source_path, 'mask_scales', cam.image_name + '.pt')\n",
    "    scales = torch.load(scale_path)\n",
    "    all_scales.append(scales)\n",
    "\n",
    "all_scales = torch.cat(all_scales)\n",
    "\n",
    "upper_bound_scale = all_scales.max().item()\n",
    "# upper_bound_scale = np.percentile(all_scales.detach().cpu().numpy(), 75)\n",
    "\n",
    "# all_scales = []\n",
    "# for cam in scene.getTrainCameras():\n",
    "#     cam.mask_scales = torch.clamp(cam.mask_scales, 0, upper_bound_scale).detach()\n",
    "#     all_scales.append(cam.mask_scales)\n",
    "# all_scales = torch.cat(all_scales)\n",
    "\n",
    "# quantile transformer\n",
    "q_trans, q_trans_ = get_quantile_func(all_scales, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a23d75-34f9-4148-aae1-0231fa745f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scales.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff6f44-47d7-4749-86ed-715040aa6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff33bf7-5e5e-4ffd-a388-1ab03d786a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trans(torch.Tensor([70]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129f8bb-3f7f-48a2-afc8-41d66af2a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "cameras = scene.getTrainCameras()\n",
    "print(\"There are\",len(cameras),\"views in the dataset.\")\n",
    "print(upper_bound_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85353c2d-386a-43b1-a8cd-dbe2387ff616",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_camera_id = 17\n",
    "mask_img_camera_id = 0\n",
    "\n",
    "view = deepcopy(cameras[ref_img_camera_id])\n",
    "\n",
    "view.feature_height, view.feature_width = view.image_height, view.image_width\n",
    "img = view.original_image * 255\n",
    "img = img.permute([1,2,0]).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_feature = render_contrastive_feature(view, feature_gaussians, pipeline.extract(args), background, norm_point_features=True, smooth_type = None)['render']\n",
    "feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfda47c-ffac-45b1-99a5-cb4725ee4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # If the q_trans is normal\n",
    "    # scale = 2.\n",
    "    # scale = torch.full((1,), scale).cuda()\n",
    "    # scale = q_trans(scale)\n",
    "\n",
    "    # If the q_trans is uniform, the scale can be any value between 0 and 1\n",
    "    # scale = torch.tensor([0]).cuda()\n",
    "    # scale = torch.tensor([0.5]).cuda()\n",
    "    scale = torch.tensor([1.5]).cuda()\n",
    "\n",
    "    gates = scale_gate(scale)\n",
    "\n",
    "    feature_with_scale = rendered_feature\n",
    "    feature_with_scale = feature_with_scale * gates.unsqueeze(-1).unsqueeze(-1)\n",
    "    scale_conditioned_feature = feature_with_scale.permute([1,2,0])\n",
    "\n",
    "    plt.imshow(scale_conditioned_feature[:,:,:3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896edcbc-1eee-4b4b-9455-5816b5e9f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = (300, 400)\n",
    "\n",
    "query_index = (\n",
    "    int(query_index[0] / view.image_height * view.feature_height),\n",
    "    int(query_index[1] / view.image_width * view.feature_width),\n",
    "               )\n",
    "\n",
    "normed_features = torch.nn.functional.normalize(scale_conditioned_feature, dim = -1, p = 2)\n",
    "query_feature = normed_features[query_index[0], query_index[1]]\n",
    "\n",
    "similarity = torch.einsum('C,HWC->HW', query_feature, normed_features)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(similarity.detach().cpu().numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(similarity.detach().cpu().numpy() > 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f397a-0f4c-4674-849f-35f0648db0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_features = torch.nn.functional.interpolate(scale_conditioned_feature.permute([2,0,1]).unsqueeze(0), (256, 256), mode = 'bilinear').squeeze()\n",
    "cluster_normed_features = torch.nn.functional.normalize(downsampled_features, dim = 0, p = 2).permute([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4fe1e-4297-4c5e-acf9-e3435bca6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=80, cluster_selection_epsilon=0.01)\n",
    "cluster_labels = clusterer.fit_predict(cluster_normed_features.reshape([-1, cluster_normed_features.shape[-1]]).detach().cpu().numpy())\n",
    "labels = cluster_labels.reshape([cluster_normed_features.shape[0], cluster_normed_features.shape[1]])\n",
    "print(np.unique(labels))\n",
    "\n",
    "cluster_centers = torch.zeros(len(np.unique(labels))-1, cluster_normed_features.shape[-1])\n",
    "for i in range(1, len(np.unique(labels))):\n",
    "    cluster_centers[i-1] = torch.nn.functional.normalize(cluster_normed_features[labels == i-1].mean(dim = 0), dim = -1)\n",
    "\n",
    "label_to_color = np.random.rand(200, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64ce33-2188-44ce-b29b-63177cb15aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_color = np.random.rand(200, 3)\n",
    "segmentation_res = torch.einsum('nc,hwc->hwn', cluster_centers.cuda(), normed_features)\n",
    "\n",
    "segmentation_res_idx = segmentation_res.argmax(dim = -1)\n",
    "colored_labels = label_to_color[segmentation_res_idx.cpu().numpy().astype(np.int8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50537b60-77e7-4086-9627-2550abcd8c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(colored_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba274092-20e8-4a70-9b3c-bab960411b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_features = feature_gaussians.get_point_features\n",
    "\n",
    "scale_conditioned_point_features = point_features * gates.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100b1b1-7b0a-40ec-89e7-65da2993a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_scale_conditioned_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "\n",
    "similarities = torch.einsum('C,NC->N', query_feature.cuda(), normed_scale_conditioned_point_features)\n",
    "\n",
    "similarities[similarities < 0.3] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcc50d5-319e-4c0d-8dfe-52fadc9a490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_similarities = render(cameras[17], scene_gaussians, pipeline.extract(args), background, override_color=similarities.unsqueeze(-1).repeat([1,3]))['render']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6be31-740d-4570-81a3-821a165e10a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rendered_similarities.permute([1,2,0])[:,:,0].detach().cpu() > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92e067-aefa-434d-85a9-9ced23748e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass\n",
    "scene_gaussians.segment(similarities > 0.48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832f4b4-1afb-4b72-aa96-adb78945d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the segmentation\n",
    "name = 'precomputed_mask'\n",
    "import os\n",
    "os.makedirs('./segmentation_res', exist_ok=True)\n",
    "torch.save(similarities > 0.48, f'./segmentation_res/{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225a97b-aa2b-4edd-abef-e2a744302e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [1 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_segmented_image = render(cameras[17], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered_segmented_image.permute([1,2,0]).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd77b9-bf00-45de-9b6c-e1eaf0168a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_gaussians.roll_back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c47f5-0cd5-41f1-80b3-c3f178b65729",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_features = feature_gaussians.get_point_features\n",
    "\n",
    "scale_conditioned_point_features = torch.nn.functional.normalize(point_features, dim = -1, p = 2) * gates.unsqueeze(0)\n",
    "\n",
    "normed_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "#niranjana\n",
    "rand_mask = torch.rand(scale_conditioned_point_features.shape[0]) > 0.98\n",
    "sampled_indices = rand_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "sampled_point_features = scale_conditioned_point_features[sampled_indices]\n",
    "normed_sampled_point_features = torch.nn.functional.normalize(sampled_point_features, dim=-1)\n",
    "#niranjana\n",
    "#sampled_point_features = scale_conditioned_point_features[torch.rand(scale_conditioned_point_features.shape[0]) > 0.98]\n",
    "\n",
    "#normed_sampled_point_features = sampled_point_features / torch.norm(sampled_point_features, dim = -1, keepdim = True)\n",
    "\n",
    "print(len(sampled_point_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6214f-6e46-4bdb-820a-281933acb9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb8330-12bf-4826-a1ee-321898b0ba04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c66c90-effe-44b5-bdcb-96dde1260a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb44d04-e02b-4995-b7b3-600ee0f1fa97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e256d-4fb8-474d-9f6f-09092987d7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77c6ca-8e07-4972-a46c-dc6996ac527a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54451c-1cbf-4727-b54f-80865f3a95e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
