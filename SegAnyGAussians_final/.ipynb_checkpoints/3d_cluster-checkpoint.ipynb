{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534d5d39-792b-48b1-afd4-6ca824f6b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import pytorch3d.ops\n",
    "from plyfile import PlyData, PlyElement\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import cv2\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "\n",
    "\n",
    "from utils.sh_utils import SH2RGB\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ece3a6-9a8e-41df-8295-1326e44e77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32 # fixed\n",
    "\n",
    "# MODEL_PATH = './output/lerf-fruit_aisle/'\n",
    "MODEL_PATH = './output/lund_1024' # 30000\n",
    "\n",
    "FEATURE_GAUSSIAN_ITERATION = 10000\n",
    "\n",
    "SCALE_GATE_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scale_gate.pt')\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876819b8-b6b2-45e9-b519-c40ef0664e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for config file in ./output/lund_1024/cfg_args\n",
      "Config file found: ./output/lund_1024/cfg_args\n",
      "Loading trained model at iteration 30000, 10000\n",
      "Allow Camera Principle Point Shift: False\n",
      "Reading camera 1196/1196\n",
      "âœ… Loaded 1196 cameras for this GPU (start_idx=0, end_idx=None)\n",
      "Loading Training Cameras\n",
      "Loading Test Cameras\n"
     ]
    }
   ],
   "source": [
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate = scale_gate.cuda()\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument('--target', default='scene', type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "\n",
    "# If use language-driven segmentation, load clip feature and original masks\n",
    "dataset.need_features = True\n",
    "\n",
    "# To obtain mask scales\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb14e35-5a94-4863-8fad-b00ee2ee34ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.245272 ?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Borrowed from GARField, but modified\n",
    "def get_quantile_func(scales: torch.Tensor, distribution=\"normal\"):\n",
    "    \"\"\"\n",
    "    Use 3D scale statistics to normalize scales -- use quantile transformer.\n",
    "    \"\"\"\n",
    "    scales = scales.flatten()\n",
    "\n",
    "    scales = scales.detach().cpu().numpy()\n",
    "    print(scales.max(), '?')\n",
    "\n",
    "    # Calculate quantile transformer\n",
    "    quantile_transformer = QuantileTransformer(output_distribution=distribution)\n",
    "    quantile_transformer = quantile_transformer.fit(scales.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    def quantile_transformer_func(scales):\n",
    "        scales_shape = scales.shape\n",
    "\n",
    "        scales = scales.reshape(-1,1)\n",
    "        \n",
    "        return torch.Tensor(\n",
    "            quantile_transformer.transform(scales.detach().cpu().numpy())\n",
    "        ).to(scales.device).reshape(scales_shape)\n",
    "\n",
    "    return quantile_transformer_func, quantile_transformer\n",
    "    \n",
    "all_scales = []\n",
    "for cam in scene.getTrainCameras():\n",
    "    scale_path = os.path.join(dataset.source_path, 'mask_scales', cam.image_name + '.pt')\n",
    "    scales = torch.load(scale_path)\n",
    "    all_scales.append(scales)\n",
    "\n",
    "all_scales = torch.cat(all_scales)\n",
    "\n",
    "upper_bound_scale = all_scales.max().item()\n",
    "# upper_bound_scale = np.percentile(all_scales.detach().cpu().numpy(), 75)\n",
    "\n",
    "# all_scales = []\n",
    "# for cam in scene.getTrainCameras():\n",
    "#     cam.mask_scales = torch.clamp(cam.mask_scales, 0, upper_bound_scale).detach()\n",
    "#     all_scales.append(cam.mask_scales)\n",
    "# all_scales = torch.cat(all_scales)\n",
    "\n",
    "# quantile transformer\n",
    "q_trans, q_trans_ = get_quantile_func(all_scales, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296503ba-f937-44ea-bd19-daf5a197fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.2453, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scales.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de824427-1794-49ff-bada-0a2160afaa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.245271682739258"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_bound_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af247da6-08ed-49b6-80cb-8dd404cb8086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_trans(torch.Tensor([70]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a9cfb1-78dc-4990-a178-491994b1f7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with torch.no_grad():\\n    # If the q_trans is normal\\n    # scale = 2.\\n    # scale = torch.full((1,), scale).cuda()\\n    # scale = q_trans(scale)\\n\\n    # If the q_trans is uniform, the scale can be any value between 0 and 1\\n    # scale = torch.tensor([0]).cuda()\\n    # scale = torch.tensor([0.5]).cuda()\\n    scale = torch.tensor([1.5]).cuda()\\n\\n    gates = scale_gate(scale)\\n\\n    #feature_with_scale = rendered_feature\\n    #feature_with_scale = feature_with_scale * gates.unsqueeze(-1).unsqueeze(-1)\\n    #scale_conditioned_feature = feature_with_scale.permute([1,2,0])\\n\\n    #plt.imshow(scale_conditioned_feature[:,:,:3].detach().cpu().numpy())'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''with torch.no_grad():\n",
    "    # If the q_trans is normal\n",
    "    # scale = 2.\n",
    "    # scale = torch.full((1,), scale).cuda()\n",
    "    # scale = q_trans(scale)\n",
    "\n",
    "    # If the q_trans is uniform, the scale can be any value between 0 and 1\n",
    "    # scale = torch.tensor([0]).cuda()\n",
    "    # scale = torch.tensor([0.5]).cuda()\n",
    "    scale = torch.tensor([1.5]).cuda()\n",
    "\n",
    "    gates = scale_gate(scale)\n",
    "\n",
    "    #feature_with_scale = rendered_feature\n",
    "    #feature_with_scale = feature_with_scale * gates.unsqueeze(-1).unsqueeze(-1)\n",
    "    #scale_conditioned_feature = feature_with_scale.permute([1,2,0])\n",
    "\n",
    "    #plt.imshow(scale_conditioned_feature[:,:,:3].detach().cpu().numpy())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec49249-3b6c-462f-ac92-19f0a5450b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Get all scales from all cameras (or just one if batching)\n",
    "    all_mask_scales = []\n",
    "    for cam in scene.getTrainCameras():\n",
    "        scale_path = os.path.join(dataset.source_path, 'mask_scales', cam.image_name + '.pt')\n",
    "        mask_scale = torch.load(scale_path).reshape(-1)\n",
    "        all_mask_scales.append(mask_scale)\n",
    "\n",
    "    all_mask_scales = torch.cat(all_mask_scales).cuda()  # Shape: [N]\n",
    "    normed_scales = q_trans(all_mask_scales).reshape(-1, 1)  # Shape: [N, 1]\n",
    "\n",
    "    # Apply scale gate\n",
    "    gate_output = scale_gate(normed_scales)               # [N, 32]\n",
    "    gate_scores = gate_output.mean(dim=1)                 # [N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b843f9-ae50-43c4-b162-120306cb5bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1196 views in the dataset.\n",
      "18.245271682739258\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "cameras = scene.getTrainCameras()\n",
    "print(\"There are\",len(cameras),\"views in the dataset.\")\n",
    "print(upper_bound_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87314e7-1a25-4587-8da1-8aec92590b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20254\n"
     ]
    }
   ],
   "source": [
    "# Get 3D Gaussian point features\n",
    "point_features = feature_gaussians.get_point_features  # [N, 32]\n",
    "\n",
    "# Get per-Gaussian scale: use norm of the 3D scale vector\n",
    "scales_3d = scene_gaussians.get_scaling  # [N, 3]\n",
    "mean_scale = scales_3d.norm(dim=1)       # [N]\n",
    "\n",
    "# Normalize using quantile transformer\n",
    "normalized_scale = q_trans(mean_scale)   # [N]\n",
    "\n",
    "# Compute scale gate values\n",
    "with torch.no_grad():\n",
    "    gate_scores = scale_gate(normalized_scale.unsqueeze(1)).mean(dim=1)  # [N]\n",
    "\n",
    "# Compute scale-aware features\n",
    "scale_conditioned_point_features = torch.nn.functional.normalize(point_features, dim=-1, p=2) * gate_scores.unsqueeze(1)\n",
    "\n",
    "normed_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "\n",
    "sample_mask = torch.rand(scale_conditioned_point_features.shape[0]) > 0.98\n",
    "sampled_point_features = scale_conditioned_point_features[sample_mask]\n",
    "normed_sampled_point_features = sampled_point_features / torch.norm(sampled_point_features, dim = -1, keepdim = True)\n",
    "\n",
    "print(len(sampled_point_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "379fbe18-ea78-49dc-beae-14ad8d41c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cluster_labels = clusterer.fit_predict(normed_sampled_point_features.detach().cpu().numpy())\\nprint(np.unique(cluster_labels))\\n\\ncluster_centers = torch.zeros(len(np.unique(cluster_labels))-1, normed_sampled_point_features.shape[-1])\\nfor i in range(1, len(np.unique(cluster_labels))):\\n    cluster_centers[i-1] = torch.nn.functional.normalize(normed_sampled_point_features[cluster_labels == i-1].mean(dim = 0), dim = -1)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, cluster_selection_epsilon=0.15)\n",
    "\n",
    "'''cluster_labels = clusterer.fit_predict(normed_sampled_point_features.detach().cpu().numpy())\n",
    "print(np.unique(cluster_labels))\n",
    "\n",
    "cluster_centers = torch.zeros(len(np.unique(cluster_labels))-1, normed_sampled_point_features.shape[-1])\n",
    "for i in range(1, len(np.unique(cluster_labels))):\n",
    "    cluster_centers[i-1] = torch.nn.functional.normalize(normed_sampled_point_features[cluster_labels == i-1].mean(dim = 0), dim = -1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "820d3750-10a0-4e6d-a19d-6ad95510fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niranjanar/miniconda3/envs/new_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/niranjanar/miniconda3/envs/new_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract features and apply PCA\n",
    "feat_array = normed_sampled_point_features.detach().cpu().numpy()\n",
    "pca_feats = PCA(n_components=20).fit_transform(feat_array)\n",
    "\n",
    "# Step 2: Run HDBSCAN on PCA-reduced features\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=80, cluster_selection_epsilon=0.1)\n",
    "cluster_labels = clusterer.fit_predict(pca_feats)\n",
    "\n",
    "# Step 3: Merge small clusters using PCA features\n",
    "min_size = 50\n",
    "label_counts = Counter(cluster_labels)\n",
    "\n",
    "# Identify small and large cluster masks\n",
    "big_mask = np.array([label_counts[lbl] >= min_size and lbl != -1 for lbl in cluster_labels])\n",
    "small_mask = ~big_mask\n",
    "\n",
    "big_feats = pca_feats[big_mask]\n",
    "big_labels = cluster_labels[big_mask]\n",
    "small_feats = pca_feats[small_mask]\n",
    "\n",
    "# Nearest neighbor matching\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(big_feats)\n",
    "_, indices = nn.kneighbors(small_feats)\n",
    "nearest_labels = big_labels[indices[:, 0]]\n",
    "\n",
    "# Apply merged labels\n",
    "merged_labels = cluster_labels.copy()\n",
    "merged_labels[small_mask] = nearest_labels\n",
    "full_labels = -np.ones(scale_conditioned_point_features.shape[0], dtype=int)\n",
    "full_labels[sample_mask.cpu().numpy()] = merged_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595d857f-c79b-4024-b5ec-77b5c80daf6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1007732] at index 0 does not match the shape of the indexed tensor [20254, 32] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cluster_centers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(full_labels))\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, normed_sampled_point_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(full_labels))):\n\u001b[0;32m----> 3\u001b[0m     cluster_centers[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(\u001b[43mnormed_sampled_point_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfull_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1007732] at index 0 does not match the shape of the indexed tensor [20254, 32] at index 0"
     ]
    }
   ],
   "source": [
    "cluster_centers = torch.zeros(len(np.unique(merged_labels))-1, normed_sampled_point_features.shape[-1])\n",
    "for i in range(1, len(np.unique(merged_labels))):\n",
    "    cluster_centers[i-1] = torch.nn.functional.normalize(normed_sampled_point_features[merged_labels == i-1].mean(dim = 0), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578e415-9fb1-4a47-983f-8a9740cc2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_score = torch.einsum('nc,bc->bn', cluster_centers.cpu(), normed_point_features.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab5b3a-fb37-4991-ae5b-a3084244af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seg_score.max().item(), seg_score.mean().item(), seg_score.min().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751a879-d7e4-4a78-90db-11cfde82f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_color = np.random.rand(1000, 3)\n",
    "point_colors = label_to_color[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "point_colors[seg_score.max(dim = -1)[0].detach().cpu().numpy() < 0.2] = (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff283c1c-6b40-4274-a0e1-b48a9c0a706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "# Inputs: assume these are numpy arrays\n",
    "positions = scene_gaussians.get_xyz.detach().cpu().numpy()      # [N, 3]\n",
    "labels = cluster_labels                                          # [N]\n",
    "\n",
    "# Optional mask\n",
    "if 'selected_mask' in locals():\n",
    "    selected_mask_np = selected_mask.cpu().numpy()\n",
    "    positions = positions[selected_mask_np]\n",
    "    labels = labels[selected_mask_np]\n",
    "\n",
    "# Create random color for each cluster\n",
    "num_clusters = int(labels.max()) + 1\n",
    "label_to_color = (np.random.rand(num_clusters + 1, 3) * 255).astype(np.uint8)\n",
    "colors = label_to_color[labels]\n",
    "\n",
    "# Build structured array for PlyElement\n",
    "vertex_data = np.array(\n",
    "    [(*pos, *color) for pos, color in zip(positions, colors)],\n",
    "    dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4'),\n",
    "           ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')]\n",
    ")\n",
    "\n",
    "ply_element = PlyElement.describe(vertex_data, 'vertex')\n",
    "PlyData([ply_element], text=True).write('clustered_output.ply')\n",
    "\n",
    "print(\"Wrote clustered_output.ply with\", len(positions), \"points.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463e241-0ec0-4402-95b1-204974148cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97737a96-232a-4488-ab23-f7f2baa41eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "\n",
    "rendered_seg_map = render(cameras[17], scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(point_colors).cuda().float())['render']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95160bd-893f-4e53-b2f9-f0fbf050bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rendered_seg_map.permute([1,2,0]).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eb747-f69b-4caa-a9c1-c1187514ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Count how many points are in each cluster\n",
    "labels = merged_labels  # already computed from HDBSCAN\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Skip noise (-1), if needed\n",
    "valid = unique_labels != -1\n",
    "unique_labels = unique_labels[valid]\n",
    "counts = counts[valid]\n",
    "\n",
    "# Sort by size (optional)\n",
    "sorted_indices = np.argsort(-counts)\n",
    "sorted_labels = unique_labels[sorted_indices]\n",
    "sorted_counts = counts[sorted_indices]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(range(len(sorted_counts)), sorted_counts)\n",
    "plt.xlabel(\"Cluster Index (sorted)\")\n",
    "plt.ylabel(\"Number of Gaussians\")\n",
    "plt.title(\"Number of Gaussians per Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820cd61-3daf-431f-ad88-7da19c07283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib notebook  # Use this only in a notebook, not a script\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "img = rendered_seg_map.permute([1, 2, 0]).detach().cpu().numpy()\n",
    "ax.imshow(img)\n",
    "\n",
    "def onclick(event):\n",
    "    if event.xdata is None or event.ydata is None:\n",
    "        return  # Ignore clicks outside image\n",
    "    x = int(event.xdata)\n",
    "    y = int(event.ydata)\n",
    "    print(f\"Clicked at: (x={x}, y={y})\")\n",
    "\n",
    "    # Compute flat index for merged_labels if it's 1D\n",
    "    idx = y * img.shape[1] + x\n",
    "    cluster_id = full_labels[idx]\n",
    "    print(f\"Cluster ID at that point: {cluster_id.item()}\")\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572b85f8-a0f0-4056-84af-d658c83b69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = torch.unique(seg_score.argmax(dim=-1))  # [num_clusters]\n",
    "normed_point_features = torch.nn.functional.normalize(feature_gaussians.get_point_features, dim=-1, p=2)\n",
    "\n",
    "cluster_features = []\n",
    "for label in unique_labels:\n",
    "    mask = (seg_score.argmax(dim=-1) == label)\n",
    "    cluster_feat = normed_point_features[mask].mean(dim=0)\n",
    "    cluster_feat = torch.nn.functional.normalize(cluster_feat, dim=0, p=2)\n",
    "    cluster_features.append(cluster_feat)\n",
    "\n",
    "cluster_features = torch.stack(cluster_features).cuda()  # [num_clusters, F]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba97be0-ee6a-46ff-8b66-c10dcf4abbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "\n",
    "def load_clip():\n",
    "    model, _ = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "    return model\n",
    "\n",
    "def encode_text(model, text):\n",
    "    tokens = clip.tokenize([text]).cuda()\n",
    "    with torch.no_grad():\n",
    "        return model.encode_text(tokens).squeeze(0)\n",
    "\n",
    "# Load CLIP\n",
    "clip_model = load_clip().eval()\n",
    "\n",
    "# Positive prompts\n",
    "positive_prompts = [\"house\", \"home\", \"residential building\", \"villa\", \"apartment\", \"building\"]\n",
    "positive_feats = [encode_text(clip_model, p).float() for p in positive_prompts]\n",
    "positive_feat = torch.stack(positive_feats).mean(dim=0)\n",
    "\n",
    "# Negative prompts\n",
    "negative_prompts = [\"trees\", \"green\",\"roads\", \"sky\", \"car\", \"grass\",\"plants\",\"bush\",\"hill\",\"garden\",\"water\",\"path\",\"street\"]\n",
    "negative_feats = [encode_text(clip_model, p).float() for p in negative_prompts]\n",
    "negative_feat = torch.stack(negative_feats).mean(dim=0)\n",
    "\n",
    "# Normalize both\n",
    "positive_feat = torch.nn.functional.normalize(positive_feat, dim=0, p=2)\n",
    "negative_feat = torch.nn.functional.normalize(negative_feat, dim=0, p=2)\n",
    "\n",
    "# Final composite feature\n",
    "text_feat = torch.nn.functional.normalize(positive_feat, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdbd0e-d5c3-402b-a78e-7c19b4f3d04c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projection = torch.nn.Linear(32, 512).cuda()\n",
    "# Project cluster features to CLIP space if needed\n",
    "projected = projection(cluster_features.float())  # Make sure cluster_features.shape = [C, 32]\n",
    "projected = torch.nn.functional.normalize(projected, dim=-1)\n",
    "# Compute similarity with prompt\n",
    "clip_scores = torch.einsum('cf,f->c', projected, text_feat)\n",
    "\n",
    "topk = 5\n",
    "topk_values, topk_indices = torch.topk(clip_scores, topk)\n",
    "\n",
    "selected_mask = torch.zeros_like(full_labels, dtype=torch.bool)\n",
    "for idx in topk_indices:\n",
    "    selected_mask |= (full_labels == idx)\n",
    "    count = (full_labels == idx).sum().item()\n",
    "    print(f\"Cluster {idx.item()} â†’ Count: {count} Gaussians\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f593de7-2c49-46ff-8324-052b7cff8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_mask = (seg_score.argmax(dim=-1) == best_label)\n",
    "scene_gaussians.segment(selected_mask)\n",
    "torch.save(selected_mask, './segmentation_res/clip_guided_cluster_segment.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e051580-1b75-4cec-9a57-45f392cbc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [1 for _ in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered = render(cameras[17], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered.permute(1, 2, 0).detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cc295-5fa3-4b4a-aa64-b111012f21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "PLY_PATH = \"path/to/contrastive_feature_point_cloud.ply\"\n",
    "SCALE_GATE_PATH = \"path/to/scale_gate.pt\"\n",
    "OUTPUT_PATH = \"filtered_point_cloud.ply\"\n",
    "\n",
    "# Load point cloud\n",
    "pcd = o3d.io.read_point_cloud(PLY_PATH)\n",
    "points = np.asarray(pcd.points)\n",
    "\n",
    "# Prepare dummy \"scales\" or any relevant feature (this may differ if original .ply had attributes)\n",
    "# Here we use distances from origin as an example feature. Replace as needed.\n",
    "distances = np.linalg.norm(points, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Load scale gate\n",
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate.eval()\n",
    "\n",
    "# Compute gate values\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor(distances, dtype=torch.float32)\n",
    "    gate_output = scale_gate(input_tensor).mean(dim=1).numpy()  # mean over 32-dim output\n",
    "\n",
    "# Thresholding\n",
    "lower_thresh = 0.3\n",
    "upper_thresh = 0.95\n",
    "mask = (gate_output >= lower_thresh) & (gate_output <= upper_thresh)\n",
    "filtered_points = points[mask]\n",
    "\n",
    "# Save filtered cloud\n",
    "filtered_pcd = o3d.geometry.PointCloud()\n",
    "filtered_pcd.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "o3d.io.write_point_cloud(OUTPUT_PATH, filtered_pcd)\n",
    "print(f\"Filtered point cloud saved to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb4291-e6f8-409d-bc34-9da197d93474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb057a4-25df-47b0-b145-7b19cee36c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc7491-56c9-4b86-969f-cc84ef799a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
